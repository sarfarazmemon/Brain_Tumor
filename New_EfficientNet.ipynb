{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE1X5Al0hwhz"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB6, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Define image size and number of classes\n",
        "IMG_SIZE = 245\n",
        "NUM_CLASSES = 4  # 4 classes: meningioma, pituitary, glioma, no_tumor\n",
        "\n",
        "# Load EfficientNetB6 with pre-trained weights and exclude the top layers for fine-tuning\n",
        "base_model = EfficientNetB6(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "# Freeze the base model to prevent training its weights\n",
        "base_model.trainable = False\n",
        "\n",
        "# Custom Layers\n",
        "model = Sequential([\n",
        "    base_model,  # Base EfficientNetB6\n",
        "    GlobalAveragePooling2D(),  # Global Average Pooling\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(NUM_CLASSES, activation='softmax')  # Output layer for 4 classes\n",
        "])\n",
        "\n",
        "# Define Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define ImageDataGenerator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training, validation, and test data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path/to/train',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    'path/to/val',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'path/to/test',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# Plot Accuracy and Loss Curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_acc:.2f}')\n",
        "\n",
        "# Predictions on the test set\n",
        "test_predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
        "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "test_true_classes = test_generator.classes\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(test_true_classes, test_pred_classes)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Calculate Precision, Recall, F1-Score, and Accuracy\n",
        "precision = precision_score(test_true_classes, test_pred_classes, average='weighted')\n",
        "recall = recall_score(test_true_classes, test_pred_classes, average='weighted')\n",
        "f1 = f1_score(test_true_classes, test_pred_classes, average='weighted')\n",
        "accuracy = accuracy_score(test_true_classes, test_pred_classes)\n",
        "\n",
        "# Specificity Calculation\n",
        "specificity = []\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    tn = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - conf_matrix[i, i])\n",
        "    fp = conf_matrix[:, i].sum() - conf_matrix[i, i]\n",
        "    specificity.append(tn / (tn + fp))\n",
        "\n",
        "# Create a DataFrame for the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'Specificity'],\n",
        "    'Value': [precision, recall, f1, accuracy, np.mean(specificity)]\n",
        "})\n",
        "\n",
        "print(metrics_df)\n",
        "\n",
        "# Save the model to an H5 file\n",
        "model.save('brain_tumor_classifier.h5')\n",
        "print(\"Model saved as brain_tumor_classifier.h5\")\n",
        "```"
      ]
    }
  ]
}